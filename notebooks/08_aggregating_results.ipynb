{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Results for Final Findings Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents the **final aggregation pipeline** for debates and media datasets in the BSc thesis:  \n",
    "`Debates, Media, and Discourse: A Computational Analysis of Temporal Shifts in U.S. Presidential Debates and Media Framing Across the Political Spectrum`, written by **Emma Cristina Mora** (emma.mora@studbocconi.it) at **Bocconi University** under the supervision of **Professor Carlo Rasmus Schwarz**.  \n",
    "\n",
    "The objective of this notebook is to **consolidate all intermediate analyses** (themes, sentiment & emotions, framing, rhetoric, and ideology) into two coherent datasets:  \n",
    "- **debates_full.csv** → utterance-level dataset for debates (1960–2024)  \n",
    "- **media_full.csv** → chunk-level dataset for media coverage (2012–2024)  \n",
    "\n",
    "This unified structure will support the **results and interpretation stage** of the thesis, enabling comparisons across domains, decades, and parties.  \n",
    "\n",
    "**Dataset Preparation**  \n",
    "- **Debates**:  \n",
    "  - Base: `debates_df_themes.csv` (utterance-level topics & subtopics)  \n",
    "  - Merged with:  \n",
    "    - `debates_frames_simple.csv` (framing labels)  \n",
    "    - `debates_sentiment_emotions.csv` (sentiment & emotions)  \n",
    "    - `debates_rhetoric.csv` (Benoit model labels)  \n",
    "    - `debates_ideology.csv` (Political Compass scores)  \n",
    "  - Harmonization: token-based `utterance_id` (hash of normalized text) used as join key.  \n",
    "  - Confidence-aware deduplication: kept rows with higher margins/probabilities when duplicates occurred.  \n",
    "  - Final schema includes: `utterance_id, year, speaker, party_code, text, theme, subtheme, sentiment, emotion, framing, rhetoric, ideology_econ, ideology_soc`.  \n",
    "\n",
    "- **Media**:  \n",
    "  - Base: `media_chunks_pred_balanced.csv` (balanced per outlet and theme)  \n",
    "  - Merged with:  \n",
    "    - `media_sentiment_emotions.csv`  \n",
    "    - `media_frames_simple.csv`  \n",
    "  - Assigned chunk-level IDs (`chunk_id`) and harmonized theme/subtheme labels.  \n",
    "  - Final schema includes: `chunk_id, year, outlet, outlet_leaning, text, source_theme, theme, subtheme, sentiment, emotion, framing`.  \n",
    "\n",
    "**Outputs**  \n",
    "- `debates_full.csv` — comprehensive debate dataset across 6,300 utterances  \n",
    "- `media_full.csv` — comprehensive balanced media dataset across 675 chunks  \n",
    "- Distribution snapshots exported separately for quick reporting:  \n",
    "  - `distribution_frames.csv`  \n",
    "  - `distribution_rhetoric.csv`  \n",
    "  - `distribution_sentiment.csv`  \n",
    "  - `distribution_emotion.csv`  \n",
    "\n",
    "**Notebook Contribution**  \n",
    "This notebook **concludes the preprocessing and aggregation stage** of the thesis by:  \n",
    "- Producing **final clean datasets** for debates and media  \n",
    "- Ensuring **alignment across analytical dimensions** (themes, sentiment, rhetoric, ideology)  \n",
    "- Creating **stable IDs and harmonized schemas** for consistent cross-domain analysis  \n",
    "\n",
    "These datasets will serve as the **foundation for the Results chapter**, where the temporal, partisan, and cross-domain dynamics of political discourse will be interpreted.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SETUP ===\n",
    "\n",
    "# standard libraries\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository Path: /Users/emmamora/Documents/GitHub/thesis\n",
      "Data Directory: /Users/emmamora/Documents/GitHub/thesis/data\n",
      "Debates Themes Path: /Users/emmamora/Documents/GitHub/thesis/data/debates_df_themes.csv\n",
      "Media Themes Path: /Users/emmamora/Documents/GitHub/thesis/data/important/media_chunks_pred_balanced.csv\n",
      "Debates Framing Path: /Users/emmamora/Documents/GitHub/thesis/data/frames/debates_frames_simple.csv\n",
      "Media Framing Path: /Users/emmamora/Documents/GitHub/thesis/data/frames/media_frames_simple.csv\n",
      "Debates Sentiment/Emotion Path: /Users/emmamora/Documents/GitHub/thesis/data/sentiment_emotion/debates_sentiment_emotions.csv\n",
      "Media Sentiment/Emotion Path: /Users/emmamora/Documents/GitHub/thesis/data/sentiment_emotion/media_sentiment_emotions.csv\n",
      "Debates Rhetoric Path: /Users/emmamora/Documents/GitHub/thesis/data/rhetoric/debates_rhetoric.csv\n",
      "Debates Ideology Path: /Users/emmamora/Documents/GitHub/thesis/data/ideological_drift/debates_ideology.csv\n"
     ]
    }
   ],
   "source": [
    "# === FILE PATHS ===\n",
    "\n",
    "# set base repository path (assumes notebook is in repo/notebooks/)\n",
    "REPO_DIR = Path(\".\").resolve().parents[0]\n",
    "\n",
    "# debates paths\n",
    "DATA_DIR = REPO_DIR / \"data\" \n",
    "DEBATES_THEMES = DATA_DIR / \"debates_df_themes.csv\"\n",
    "DEBATES_FRAMING = DATA_DIR / \"frames\" / \"debates_frames_simple.csv\"\n",
    "DEBATES_SENTIMENT_EMOTION = DATA_DIR / \"sentiment_emotion\" / \"debates_sentiment_emotions.csv\"\n",
    "DEBATES_RHETORIC = DATA_DIR / \"rhetoric\" / \"debates_rhetoric.csv\"\n",
    "DEBATES_IDEOLOGY = DATA_DIR / \"ideological_drift\" / \"debates_ideology.csv\"\n",
    "\n",
    "# media paths\n",
    "MEDIA_THEMES = DATA_DIR / \"important\" / \"media_chunks_pred_balanced.csv\"\n",
    "MEDIA_FRAMING = DATA_DIR / \"frames\" / \"media_frames_simple.csv\"\n",
    "MEDIA_SENTIMENT_EMOTION = DATA_DIR / \"sentiment_emotion\" / \"media_sentiment_emotions.csv\"\n",
    "\n",
    "# confirm setup\n",
    "print(\"Repository Path:\", REPO_DIR)\n",
    "print(\"Data Directory:\", DATA_DIR)\n",
    "print(\"Debates Themes Path:\", DEBATES_THEMES)\n",
    "print(\"Media Themes Path:\", MEDIA_THEMES)\n",
    "print(\"Debates Framing Path:\", DEBATES_FRAMING)\n",
    "print(\"Media Framing Path:\", MEDIA_FRAMING)\n",
    "print(\"Debates Sentiment/Emotion Path:\", DEBATES_SENTIMENT_EMOTION)\n",
    "print(\"Media Sentiment/Emotion Path:\", MEDIA_SENTIMENT_EMOTION)\n",
    "print(\"Debates Rhetoric Path:\", DEBATES_RHETORIC)\n",
    "print(\"Debates Ideology Path:\", DEBATES_IDEOLOGY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Debates_Full Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HELPER FUNCTIONS ===\n",
    "\n",
    "# normalize and hash text into a stable token-based uid\n",
    "import hashlib\n",
    "\n",
    "def _normalize_text_for_uid(s: str) -> str:\n",
    "# lowercase, strip punctuation (keep apostrophes), collapse whitespace\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^\\w\\s']\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def token_uid(text: str) -> str:\n",
    "# sha1 hash of normalized text to create a deterministic uid\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    norm = _normalize_text_for_uid(str(text))\n",
    "    return hashlib.sha1(norm.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def make_text_index(s: str, k: int = 7) -> str:\n",
    "# short human-readable index from first k words (optional debugging)\n",
    "    if pd.isna(s):\n",
    "        return np.nan\n",
    "    s = _normalize_text_for_uid(str(s))\n",
    "    return \" \".join(s.split()[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "themes: (6316, 22)\n",
      "frames: (6316, 28)\n",
      "sentiment: (6316, 10)\n",
      "rhetoric: (6316, 10)\n",
      "ideology: (6316, 14)\n"
     ]
    }
   ],
   "source": [
    "# === LOAD DEBATES DATA ===\n",
    "\n",
    "# load the debates datasets\n",
    "themes = pd.read_csv(DEBATES_THEMES)\n",
    "frames = pd.read_csv(DEBATES_FRAMING)\n",
    "sentiment = pd.read_csv(DEBATES_SENTIMENT_EMOTION)\n",
    "rhetoric = pd.read_csv(DEBATES_RHETORIC)\n",
    "ideology = pd.read_csv(DEBATES_IDEOLOGY)\n",
    "\n",
    "# add token_uid and a human-friendly text_index to each df\n",
    "for df in [themes, frames, sentiment, rhetoric, ideology]:\n",
    "    if \"text\" in df.columns:\n",
    "        df[\"token_uid\"] = df[\"text\"].map(token_uid)\n",
    "        df[\"text_index\"] = df[\"text\"].map(make_text_index)\n",
    "    else:\n",
    "        df[\"token_uid\"] = np.nan\n",
    "        df[\"text_index\"] = np.nan\n",
    "\n",
    "print(\"themes:\", themes.shape)\n",
    "print(\"frames:\", frames.shape)\n",
    "print(\"sentiment:\", sentiment.shape)\n",
    "print(\"rhetoric:\", rhetoric.shape)\n",
    "print(\"ideology:\", ideology.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       debates_df_themes.csv | rows=   6316 | null_uid=     0 | dup_uid_rows=    44 | unique_uids=   6293\n",
      "   debates_frames_simple.csv | rows=   6316 | null_uid=     0 | dup_uid_rows=    44 | unique_uids=   6293\n",
      "debates_sentiment_emotions.csv | rows=   6316 | null_uid=     0 | dup_uid_rows=    44 | unique_uids=   6293\n",
      "        debates_rhetoric.csv | rows=   6316 | null_uid=     0 | dup_uid_rows=    44 | unique_uids=   6293\n",
      "        debates_ideology.csv | rows=   6316 | null_uid=     0 | dup_uid_rows=    44 | unique_uids=   6293\n"
     ]
    }
   ],
   "source": [
    "# === QUICK SANITY CHECKS ON KEYS ===\n",
    "\n",
    "# check nulls and duplicates of token_uid per dataset\n",
    "def key_report(name, df):\n",
    "    n = len(df)\n",
    "    nulls = int(df[\"token_uid\"].isna().sum())\n",
    "    dups = int(df[\"token_uid\"].duplicated(keep=False).sum())\n",
    "    uniq = int(df[\"token_uid\"].nunique(dropna=True))\n",
    "    print(f\"{name:>28} | rows={n:>7} | null_uid={nulls:>6} | dup_uid_rows={dups:>6} | unique_uids={uniq:>7}\")\n",
    "\n",
    "for name, df in [\n",
    "    (\"debates_df_themes.csv\", themes),\n",
    "    (\"debates_frames_simple.csv\", frames),\n",
    "    (\"debates_sentiment_emotions.csv\", sentiment),\n",
    "    (\"debates_rhetoric.csv\", rhetoric),\n",
    "    (\"debates_ideology.csv\", ideology),\n",
    "]:\n",
    "    key_report(name, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base rows: 6316\n"
     ]
    }
   ],
   "source": [
    "# === PREPARE BASE DATA (THEMES) ===\n",
    "\n",
    "# select base columns from themes and keep token_uid as the primary merge key\n",
    "base_cols = [\n",
    "    \"token_uid\", \"utterance_id\", \"year\", \"speaker\", \"party_code\",\n",
    "    \"text\", \"token_count\", \"theme_name\", \"subtheme_name\", \"debate_id\"\n",
    "]\n",
    "base = themes[base_cols].copy()\n",
    "\n",
    "# extract debate_number from debate_id when available\n",
    "base[\"debate_number\"] = base[\"debate_id\"].astype(str).str.extract(r\"(\\d+)\")\n",
    "\n",
    "print(\"base rows:\", base.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEDUP AUX TABLES ON TOKEN_UID (CONFIDENCE-AWARE WHERE POSSIBLE) ===\n",
    "\n",
    "# helper to drop duplicates on token_uid keeping the most confident row when scores exist\n",
    "def dedupe_on_uid(df, sort_cols=None, ascending=None):\n",
    "    if sort_cols is not None:\n",
    "        df = df.sort_values(by=sort_cols, ascending=ascending, na_position=\"last\")\n",
    "# keep the first row per uid after sorting (i.e., highest confidence)\n",
    "    return df.drop_duplicates(subset=[\"token_uid\"], keep=\"first\")\n",
    "\n",
    "# frames -> prefer higher margin, then higher best_score\n",
    "frames_cols_keep = [\"token_uid\", \"frame_final\", \"best_score\", \"margin\"]\n",
    "frames_dedup = dedupe_on_uid(\n",
    "    frames[frames_cols_keep].copy(),\n",
    "    sort_cols=[\"margin\", \"best_score\"], ascending=[False, False]\n",
    ")\n",
    "\n",
    "# sentiment -> prefer higher sentiment_score, then higher emotion_score\n",
    "sent_cols_keep = [\"token_uid\", \"sentiment_final\", \"sentiment_score\", \"emotion_final\", \"emotion_score\"]\n",
    "sentiment_dedup = dedupe_on_uid(\n",
    "    sentiment[sent_cols_keep].copy(),\n",
    "    sort_cols=[\"sentiment_score\", \"emotion_score\"], ascending=[False, False]\n",
    ")\n",
    "\n",
    "# rhetoric -> no explicit score columns, just drop duplicate uids\n",
    "rhet_cols_keep = [\"token_uid\", \"rhetoric_label\"]\n",
    "rhetoric_dedup = dedupe_on_uid(rhetoric[rhet_cols_keep].copy())\n",
    "\n",
    "# ideology -> prefer rows with both econ and soc present, then smaller std if available\n",
    "ideo = ideology.copy()\n",
    "ideo[\"has_both\"] = ideo[\"econ\"].notna().astype(int) + ideo[\"soc\"].notna().astype(int)\n",
    "if {\"econ_std\", \"soc_std\"}.issubset(ideo.columns):\n",
    "    ideo[\"std_sum\"] = ideo[[\"econ_std\", \"soc_std\"]].sum(axis=1, min_count=1)\n",
    "    sort_cols = [\"has_both\", \"std_sum\"]\n",
    "    ascending = [False, True]\n",
    "else:\n",
    "    sort_cols = [\"has_both\"]\n",
    "    ascending = [False]\n",
    "\n",
    "ideo_cols_keep = [\"token_uid\", \"econ\", \"soc\"]\n",
    "if \"econ_std\" in ideo.columns: ideo_cols_keep.append(\"econ_std\")\n",
    "if \"soc_std\" in ideo.columns: ideo_cols_keep.append(\"soc_std\")\n",
    "\n",
    "ideology_dedup = dedupe_on_uid(\n",
    "    ideo[ideo_cols_keep + ([\"has_both\", \"std_sum\"] if \"std_sum\" in ideo.columns else [\"has_both\"])].copy(),\n",
    "    sort_cols=sort_cols, ascending=ascending\n",
    ")[[\"token_uid\", \"econ\", \"soc\"]]# trim helper cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged rows: 6316\n"
     ]
    }
   ],
   "source": [
    "# === MERGE ON TOKEN_UID ===\n",
    "\n",
    "# start from base (themes) and left-join the other tables by token_uid\n",
    "merged = base.merge(\n",
    "    sentiment_dedup[[\"token_uid\", \"sentiment_final\", \"emotion_final\"]],\n",
    "    on=\"token_uid\", how=\"left\"\n",
    ").merge(\n",
    "    frames_dedup[[\"token_uid\", \"frame_final\"]],\n",
    "    on=\"token_uid\", how=\"left\"\n",
    ").merge(\n",
    "    rhetoric_dedup[[\"token_uid\", \"rhetoric_label\"]],\n",
    "    on=\"token_uid\", how=\"left\"\n",
    ").merge(\n",
    "    ideology_dedup[[\"token_uid\", \"econ\", \"soc\"]],\n",
    "    on=\"token_uid\", how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"merged rows:\", merged.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final debates_full shape: (6316, 15)\n"
     ]
    }
   ],
   "source": [
    "# === CLEAN / RENAME COLUMNS TO FINAL SCHEMA ===\n",
    "\n",
    "# rename to final names and map token_uid to the new utterance_id\n",
    "merged = merged.rename(columns={\n",
    "    \"token_uid\": \"utterance_id\",# new, reliable id based on text tokens\n",
    "    \"theme_name\": \"theme\",\n",
    "    \"subtheme_name\": \"subtheme\",\n",
    "    \"sentiment_final\": \"sentiment\",\n",
    "    \"emotion_final\": \"emotion\",\n",
    "    \"frame_final\": \"framing\",\n",
    "    \"rhetoric_label\": \"rhetoric\",\n",
    "    \"econ\": \"ideology_econ\",\n",
    "    \"soc\": \"ideology_soc\"\n",
    "})\n",
    "\n",
    "# select and order final columns\n",
    "final_cols = [\n",
    "    \"utterance_id\", \"year\", \"speaker\", \"party_code\",\n",
    "    \"text\", \"token_count\", \"theme\", \"subtheme\",\n",
    "    \"sentiment\", \"emotion\", \"framing\", \"rhetoric\",\n",
    "    \"ideology_econ\", \"ideology_soc\"\n",
    "]\n",
    "debates_full = merged[final_cols].copy()\n",
    "\n",
    "print(\"final debates_full shape:\", debates_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>utterance_id</th>\n",
       "      <th>year</th>\n",
       "      <th>speaker</th>\n",
       "      <th>party_code</th>\n",
       "      <th>text</th>\n",
       "      <th>token_count</th>\n",
       "      <th>theme</th>\n",
       "      <th>subtheme</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>emotion</th>\n",
       "      <th>framing</th>\n",
       "      <th>rhetoric</th>\n",
       "      <th>ideology_econ</th>\n",
       "      <th>ideology_soc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f25967e9ebde6a67d7e07f5e1f6dab708827bdfd</td>\n",
       "      <td>1960_1_Presidential_Nixon_Kennedy_001</td>\n",
       "      <td>1960</td>\n",
       "      <td>Moderator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good evening. the television and radio station...</td>\n",
       "      <td>146</td>\n",
       "      <td>debate_format_procedure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>joy</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263f28271313bb0ea8d815486ae0b9bfc5403e66</td>\n",
       "      <td>1960_1_Presidential_Nixon_Kennedy_002</td>\n",
       "      <td>1960</td>\n",
       "      <td>Kennedy</td>\n",
       "      <td>D</td>\n",
       "      <td>mr. smith, mr. nixon. in the election of 1860,...</td>\n",
       "      <td>1290</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>historical_foreign_policy_cuba_vietnam</td>\n",
       "      <td>neutral</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>acclaim</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b0d8772ca1e4a0ef0bf21bbc46824a6b4f888812</td>\n",
       "      <td>1960_1_Presidential_Nixon_Kennedy_004</td>\n",
       "      <td>1960</td>\n",
       "      <td>Nixon</td>\n",
       "      <td>R</td>\n",
       "      <td>mr. smith, senator kennedy. the things that se...</td>\n",
       "      <td>1406</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>historical_foreign_policy_cuba_vietnam</td>\n",
       "      <td>neutral</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>defense</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d67300aa39ec063e5dd36b5a05a0ef3121909386</td>\n",
       "      <td>1960_1_Presidential_Nixon_Kennedy_005</td>\n",
       "      <td>1960</td>\n",
       "      <td>Moderator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thank you, mr. nixon. that completes the openi...</td>\n",
       "      <td>66</td>\n",
       "      <td>debate_format_procedure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>joy</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f3cc3ad77b299c9d5dbf579b752f7af515d96397</td>\n",
       "      <td>1960_1_Presidential_Nixon_Kennedy_006</td>\n",
       "      <td>1960</td>\n",
       "      <td>Moderator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>senator, the vice president in his campaign ha...</td>\n",
       "      <td>41</td>\n",
       "      <td>leadership_executive_experience</td>\n",
       "      <td>social_security_pensions</td>\n",
       "      <td>negative</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6311</th>\n",
       "      <td>b895f43a4439e264fdef041836fa2abeff861ae8</td>\n",
       "      <td>2024_3_Vice_presidential_Vance_Walz_136</td>\n",
       "      <td>2024</td>\n",
       "      <td>Walz</td>\n",
       "      <td>D</td>\n",
       "      <td>please. yeah, well, i don't run facebook. what...</td>\n",
       "      <td>200</td>\n",
       "      <td>partisan_gridlock_new_leadership</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>morality and ethics</td>\n",
       "      <td>attack</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6312</th>\n",
       "      <td>3682ec0098e410d4ed834a526257b7c5eafd5a26</td>\n",
       "      <td>2024_3_Vice_presidential_Vance_Walz_137</td>\n",
       "      <td>2024</td>\n",
       "      <td>Moderator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>governor, your time is up. thank you, gentleme...</td>\n",
       "      <td>68</td>\n",
       "      <td>debate_format_procedure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>joy</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6313</th>\n",
       "      <td>0effb049509ae9b2d35a557d8ff3f6cdb4127928</td>\n",
       "      <td>2024_3_Vice_presidential_Vance_Walz_138</td>\n",
       "      <td>2024</td>\n",
       "      <td>Walz</td>\n",
       "      <td>D</td>\n",
       "      <td>well, thank you, senator vance. thank you to c...</td>\n",
       "      <td>404</td>\n",
       "      <td>partisan_gridlock_new_leadership</td>\n",
       "      <td>NaN</td>\n",
       "      <td>positive</td>\n",
       "      <td>joy</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>acclaim</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6314</th>\n",
       "      <td>0f98f9bc9f9ff512fd2685b214122a7dcc8bd853</td>\n",
       "      <td>2024_3_Vice_presidential_Vance_Walz_140</td>\n",
       "      <td>2024</td>\n",
       "      <td>Vance</td>\n",
       "      <td>R</td>\n",
       "      <td>well, i want to thank governor walz, you folks...</td>\n",
       "      <td>474</td>\n",
       "      <td>noise_or_unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "      <td>joy</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>attack</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6315</th>\n",
       "      <td>65d9219c480cb1e5512dd69ce7bdece3ce549195</td>\n",
       "      <td>2024_3_Vice_presidential_Vance_Walz_141</td>\n",
       "      <td>2024</td>\n",
       "      <td>Moderator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>senator vance, thank you. and thank you both f...</td>\n",
       "      <td>57</td>\n",
       "      <td>debate_format_procedure</td>\n",
       "      <td>debate_opening_closing_remarks</td>\n",
       "      <td>positive</td>\n",
       "      <td>joy</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6316 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  utterance_id  \\\n",
       "0     f25967e9ebde6a67d7e07f5e1f6dab708827bdfd   \n",
       "1     263f28271313bb0ea8d815486ae0b9bfc5403e66   \n",
       "2     b0d8772ca1e4a0ef0bf21bbc46824a6b4f888812   \n",
       "3     d67300aa39ec063e5dd36b5a05a0ef3121909386   \n",
       "4     f3cc3ad77b299c9d5dbf579b752f7af515d96397   \n",
       "...                                        ...   \n",
       "6311  b895f43a4439e264fdef041836fa2abeff861ae8   \n",
       "6312  3682ec0098e410d4ed834a526257b7c5eafd5a26   \n",
       "6313  0effb049509ae9b2d35a557d8ff3f6cdb4127928   \n",
       "6314  0f98f9bc9f9ff512fd2685b214122a7dcc8bd853   \n",
       "6315  65d9219c480cb1e5512dd69ce7bdece3ce549195   \n",
       "\n",
       "                                 utterance_id  year    speaker party_code  \\\n",
       "0       1960_1_Presidential_Nixon_Kennedy_001  1960  Moderator        NaN   \n",
       "1       1960_1_Presidential_Nixon_Kennedy_002  1960    Kennedy          D   \n",
       "2       1960_1_Presidential_Nixon_Kennedy_004  1960      Nixon          R   \n",
       "3       1960_1_Presidential_Nixon_Kennedy_005  1960  Moderator        NaN   \n",
       "4       1960_1_Presidential_Nixon_Kennedy_006  1960  Moderator        NaN   \n",
       "...                                       ...   ...        ...        ...   \n",
       "6311  2024_3_Vice_presidential_Vance_Walz_136  2024       Walz          D   \n",
       "6312  2024_3_Vice_presidential_Vance_Walz_137  2024  Moderator        NaN   \n",
       "6313  2024_3_Vice_presidential_Vance_Walz_138  2024       Walz          D   \n",
       "6314  2024_3_Vice_presidential_Vance_Walz_140  2024      Vance          R   \n",
       "6315  2024_3_Vice_presidential_Vance_Walz_141  2024  Moderator        NaN   \n",
       "\n",
       "                                                   text  token_count  \\\n",
       "0     good evening. the television and radio station...          146   \n",
       "1     mr. smith, mr. nixon. in the election of 1860,...         1290   \n",
       "2     mr. smith, senator kennedy. the things that se...         1406   \n",
       "3     thank you, mr. nixon. that completes the openi...           66   \n",
       "4     senator, the vice president in his campaign ha...           41   \n",
       "...                                                 ...          ...   \n",
       "6311  please. yeah, well, i don't run facebook. what...          200   \n",
       "6312  governor, your time is up. thank you, gentleme...           68   \n",
       "6313  well, thank you, senator vance. thank you to c...          404   \n",
       "6314  well, i want to thank governor walz, you folks...          474   \n",
       "6315  senator vance, thank you. and thank you both f...           57   \n",
       "\n",
       "                                 theme  \\\n",
       "0              debate_format_procedure   \n",
       "1     foreign_policy_national_security   \n",
       "2     foreign_policy_national_security   \n",
       "3              debate_format_procedure   \n",
       "4      leadership_executive_experience   \n",
       "...                                ...   \n",
       "6311  partisan_gridlock_new_leadership   \n",
       "6312           debate_format_procedure   \n",
       "6313  partisan_gridlock_new_leadership   \n",
       "6314              noise_or_unspecified   \n",
       "6315           debate_format_procedure   \n",
       "\n",
       "                                    subtheme sentiment      emotion  \\\n",
       "0                                        NaN  positive          joy   \n",
       "1     historical_foreign_policy_cuba_vietnam   neutral  unspecified   \n",
       "2     historical_foreign_policy_cuba_vietnam   neutral  unspecified   \n",
       "3                                        NaN  positive          joy   \n",
       "4                   social_security_pensions  negative  unspecified   \n",
       "...                                      ...       ...          ...   \n",
       "6311                                     NaN  negative  unspecified   \n",
       "6312                                     NaN  positive          joy   \n",
       "6313                                     NaN  positive          joy   \n",
       "6314                                     NaN   neutral          joy   \n",
       "6315          debate_opening_closing_remarks  positive          joy   \n",
       "\n",
       "                  framing rhetoric  ideology_econ  ideology_soc  \n",
       "0             unspecified      NaN            NaN           NaN  \n",
       "1             unspecified  acclaim          -0.45          0.05  \n",
       "2             unspecified  defense           0.55          0.10  \n",
       "3             unspecified      NaN            NaN           NaN  \n",
       "4             unspecified      NaN            NaN           NaN  \n",
       "...                   ...      ...            ...           ...  \n",
       "6311  morality and ethics   attack           0.00          0.40  \n",
       "6312          unspecified      NaN            NaN           NaN  \n",
       "6313          unspecified  acclaim          -0.60         -0.40  \n",
       "6314          unspecified   attack           0.50          0.20  \n",
       "6315          unspecified      NaN            NaN           NaN  \n",
       "\n",
       "[6316 rows x 15 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debates_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base rows: 6316\n",
      "debates_full rows: 6316\n",
      "missing fields: {'sentiment_missing': 0, 'emotion_missing': 0, 'framing_missing': 0, 'rhetoric_missing': 1552, 'ideology_econ_missing': 2473, 'ideology_soc_missing': 2473}\n"
     ]
    }
   ],
   "source": [
    "# === VALIDATION SNAPSHOT ===\n",
    "\n",
    "# quick checks to ensure we didn't lose base rows and to inspect missing rates\n",
    "print(\"base rows:\", len(base))\n",
    "print(\"debates_full rows:\", len(debates_full))\n",
    "\n",
    "missing_report = {\n",
    "    \"sentiment_missing\": int(debates_full[\"sentiment\"].isna().sum()),\n",
    "    \"emotion_missing\": int(debates_full[\"emotion\"].isna().sum()),\n",
    "    \"framing_missing\": int(debates_full[\"framing\"].isna().sum()),\n",
    "    \"rhetoric_missing\": int(debates_full[\"rhetoric\"].isna().sum()),\n",
    "    \"ideology_econ_missing\": int(debates_full[\"ideology_econ\"].isna().sum()),\n",
    "    \"ideology_soc_missing\": int(debates_full[\"ideology_soc\"].isna().sum()),\n",
    "}\n",
    "print(\"missing fields:\", missing_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /Users/emmamora/Documents/GitHub/thesis/data/debates_full.csv\n"
     ]
    }
   ],
   "source": [
    "# === SAVE DEBATES_FULL ===\n",
    "\n",
    "# write final merged file\n",
    "OUTPUT_PATH = DATA_DIR / \"debates_full.csv\"\n",
    "debates_full.to_csv(OUTPUT_PATH, index=False)\n",
    "print(\"saved:\", OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved distributions:\n",
      " - /Users/emmamora/Documents/GitHub/thesis/data/distribution_frames.csv\n",
      " - /Users/emmamora/Documents/GitHub/thesis/data/distribution_rhetoric.csv\n",
      " - /Users/emmamora/Documents/GitHub/thesis/data/distribution_sentiment.csv\n",
      " - /Users/emmamora/Documents/GitHub/thesis/data/distribution_emotion.csv\n"
     ]
    }
   ],
   "source": [
    "# === SAVE DISTRIBUTIONS ===\n",
    "\n",
    "# compute and save distributions to csv for quick reporting\n",
    "dist_frames = frames_dedup[\"frame_final\"].value_counts(dropna=False).reset_index()\n",
    "dist_frames.columns = [\"frame\", \"count\"]\n",
    "dist_frames_path = DATA_DIR / \"distribution_frames.csv\"\n",
    "dist_frames.to_csv(dist_frames_path, index=False)\n",
    "\n",
    "dist_rhetoric = rhetoric_dedup[\"rhetoric_label\"].value_counts(dropna=False).reset_index()\n",
    "dist_rhetoric.columns = [\"rhetoric\", \"count\"]\n",
    "dist_rhetoric_path = DATA_DIR / \"distribution_rhetoric.csv\"\n",
    "dist_rhetoric.to_csv(dist_rhetoric_path, index=False)\n",
    "\n",
    "dist_sentiment = sentiment_dedup[\"sentiment_final\"].value_counts(dropna=False).reset_index()\n",
    "dist_sentiment.columns = [\"sentiment\", \"count\"]\n",
    "dist_sentiment_path = DATA_DIR / \"distribution_sentiment.csv\"\n",
    "dist_sentiment.to_csv(dist_sentiment_path, index=False)\n",
    "\n",
    "dist_emotion = sentiment_dedup[\"emotion_final\"].value_counts(dropna=False).reset_index()\n",
    "dist_emotion.columns = [\"emotion\", \"count\"]\n",
    "dist_emotion_path = DATA_DIR / \"distribution_emotion.csv\"\n",
    "dist_emotion.to_csv(dist_emotion_path, index=False)\n",
    "\n",
    "print(\"saved distributions:\")\n",
    "print(\" -\", dist_frames_path)\n",
    "print(\" -\", dist_rhetoric_path)\n",
    "print(\" -\", dist_sentiment_path)\n",
    "print(\" -\", dist_emotion_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Media_Balanced Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media Chunks Shape: (675, 20)\n",
      "Media Sentiment Shape: (675, 8)\n",
      "Media Frames Shape: (675, 25)\n"
     ]
    }
   ],
   "source": [
    "# === LOAD MEDIA DATA ===\n",
    "\n",
    "# load base chunks, sentiment/emotion, and framing\n",
    "media_chunks = pd.read_csv(MEDIA_THEMES)\n",
    "media_sentiment = pd.read_csv(MEDIA_SENTIMENT_EMOTION)\n",
    "media_frames = pd.read_csv(MEDIA_FRAMING)\n",
    "\n",
    "print(\"Media Chunks Shape:\", media_chunks.shape)\n",
    "print(\"Media Sentiment Shape:\", media_sentiment.shape)\n",
    "print(\"Media Frames Shape:\", media_frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE ROW IDS FOR ALIGNMENT ===\n",
    "\n",
    "# assign artificial row numbers\n",
    "media_chunks = media_chunks.reset_index(drop=True)\n",
    "media_sentiment = media_sentiment.reset_index(drop=True)\n",
    "media_frames = media_frames.reset_index(drop=True)\n",
    "\n",
    "media_chunks[\"row_id\"] = media_chunks.index + 1\n",
    "media_sentiment[\"row_id\"] = media_sentiment.index + 1\n",
    "media_frames[\"row_id\"] = media_frames.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREPARE BASE MEDIA ===\n",
    "\n",
    "# keep relevant columns and rename text\n",
    "media = media_chunks.rename(columns={\"chunk_text\": \"text\"})\n",
    "\n",
    "# assign theme\n",
    "media[\"theme\"] = media.apply(\n",
    "    lambda r: r[\"pred_theme\"] if r[\"pred_sim\"] > 0.5 else \"noise_or_unspecified\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# assign subtheme\n",
    "media[\"subtheme\"] = media.apply(\n",
    "    lambda r: r[\"pred_subtheme\"]\n",
    "    if (r[\"pred_sub_sim\"] > 0.5 and r[\"theme\"] != \"noise_or_unspecified\")\n",
    "    else pd.NA,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# keep base columns\n",
    "media = media[[\"row_id\", \"year\", \"outlet\", \"outlet_leaning\", \"text\", \"source_theme\", \"theme\", \"subtheme\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MERGE SENTIMENT, EMOTION, FRAMING ===\n",
    "\n",
    "# sentiment + emotion\n",
    "media = media.merge(\n",
    "    media_sentiment[[\"row_id\", \"sentiment_final\", \"emotion_final\"]],\n",
    "    on=\"row_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# framing\n",
    "media = media.merge(\n",
    "    media_frames[[\"row_id\", \"frame_final\"]],\n",
    "    on=\"row_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# rename\n",
    "media = media.rename(columns={\n",
    "    \"sentiment_final\": \"sentiment\",\n",
    "    \"emotion_final\": \"emotion\",\n",
    "    \"frame_final\": \"framing\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>year</th>\n",
       "      <th>outlet</th>\n",
       "      <th>outlet_leaning</th>\n",
       "      <th>text</th>\n",
       "      <th>source_theme</th>\n",
       "      <th>theme</th>\n",
       "      <th>subtheme</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>emotion</th>\n",
       "      <th>framing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>med_000001</td>\n",
       "      <td>2016</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>stone said. jonathan gruber, the mit professor...</td>\n",
       "      <td>healthcare_public_health</td>\n",
       "      <td>healthcare_social_security</td>\n",
       "      <td>affordable_care_health_insurance</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>med_000002</td>\n",
       "      <td>2016</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>\"we're thinking of having him in the spin room...</td>\n",
       "      <td>healthcare_public_health</td>\n",
       "      <td>healthcare_social_security</td>\n",
       "      <td>affordable_care_health_insurance</td>\n",
       "      <td>neutral</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>economic consequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>med_000003</td>\n",
       "      <td>2024</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>iowa caucuses. trump was right when he accused...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>judiciary_supreme_court</td>\n",
       "      <td>abortion_constitutional_amendments</td>\n",
       "      <td>neutral</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>med_000004</td>\n",
       "      <td>2016</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>if the government stops fighting a lawsuit tha...</td>\n",
       "      <td>healthcare_public_health</td>\n",
       "      <td>healthcare_social_security</td>\n",
       "      <td>affordable_care_health_insurance</td>\n",
       "      <td>negative</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>economic consequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>med_000005</td>\n",
       "      <td>2020</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>foreign desk: stopping the ayatollahs' nukes n...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>iran_nuclear_program</td>\n",
       "      <td>neutral</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>economic consequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>med_000006</td>\n",
       "      <td>2024</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>threats.\" two days later, the russians request...</td>\n",
       "      <td>healthcare_public_health</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>russia_soviet_union</td>\n",
       "      <td>negative</td>\n",
       "      <td>anger</td>\n",
       "      <td>unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>med_000007</td>\n",
       "      <td>2024</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>two days later, the russians requested another...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>russia_soviet_union</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     chunk_id  year outlet outlet_leaning  \\\n",
       "0  med_000001  2016    nyp              R   \n",
       "1  med_000002  2016    nyp              R   \n",
       "2  med_000003  2024    nyp              R   \n",
       "3  med_000004  2016    nyp              R   \n",
       "4  med_000005  2020    nyp              R   \n",
       "5  med_000006  2024    nyp              R   \n",
       "6  med_000007  2024    nyp              R   \n",
       "\n",
       "                                                text  \\\n",
       "0  stone said. jonathan gruber, the mit professor...   \n",
       "1  \"we're thinking of having him in the spin room...   \n",
       "2  iowa caucuses. trump was right when he accused...   \n",
       "3  if the government stops fighting a lawsuit tha...   \n",
       "4  foreign desk: stopping the ayatollahs' nukes n...   \n",
       "5  threats.\" two days later, the russians request...   \n",
       "6  two days later, the russians requested another...   \n",
       "\n",
       "                       source_theme                             theme  \\\n",
       "0          healthcare_public_health        healthcare_social_security   \n",
       "1          healthcare_public_health        healthcare_social_security   \n",
       "2  foreign_policy_national_security           judiciary_supreme_court   \n",
       "3          healthcare_public_health        healthcare_social_security   \n",
       "4  foreign_policy_national_security  foreign_policy_national_security   \n",
       "5          healthcare_public_health  foreign_policy_national_security   \n",
       "6  foreign_policy_national_security  foreign_policy_national_security   \n",
       "\n",
       "                             subtheme    sentiment      emotion  \\\n",
       "0    affordable_care_health_insurance  unspecified  unspecified   \n",
       "1    affordable_care_health_insurance      neutral  unspecified   \n",
       "2  abortion_constitutional_amendments      neutral  unspecified   \n",
       "3    affordable_care_health_insurance     negative  unspecified   \n",
       "4                iran_nuclear_program      neutral  unspecified   \n",
       "5                 russia_soviet_union     negative        anger   \n",
       "6                 russia_soviet_union  unspecified  unspecified   \n",
       "\n",
       "                 framing  \n",
       "0            unspecified  \n",
       "1  economic consequences  \n",
       "2            unspecified  \n",
       "3  economic consequences  \n",
       "4  economic consequences  \n",
       "5            unspecified  \n",
       "6            unspecified  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === FINALIZE MEDIA FULL ===\n",
    "\n",
    "# drop helper row_id\n",
    "media = media.reset_index(drop=True)\n",
    "media[\"chunk_id\"] = media.index + 1\n",
    "media[\"chunk_id\"] = media[\"chunk_id\"].apply(lambda x: f\"med_{x:06d}\")\n",
    "\n",
    "# reorder columns\n",
    "media_final = media[[\n",
    "    \"chunk_id\", \"year\", \"outlet\", \"outlet_leaning\", \"text\",\n",
    "    \"source_theme\", \"theme\", \"subtheme\", \"sentiment\", \"emotion\", \"framing\"\n",
    "]]\n",
    "\n",
    "print(media_final.shape)\n",
    "media_final.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated media_full.csv with new chunk_id: /Users/emmamora/Documents/GitHub/thesis/data/media_full.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>year</th>\n",
       "      <th>outlet</th>\n",
       "      <th>outlet_leaning</th>\n",
       "      <th>text</th>\n",
       "      <th>source_theme</th>\n",
       "      <th>theme</th>\n",
       "      <th>subtheme</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>emotion</th>\n",
       "      <th>framing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012_nyp_foreign_001</td>\n",
       "      <td>2012</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>the debate, in boca raton, fla., is supposed t...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>patriot_act_homeland_security</td>\n",
       "      <td>neutral</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>security and safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012_nyp_foreign_002</td>\n",
       "      <td>2012</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>the debate, in boca raton, fla., is supposed t...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>patriot_act_homeland_security</td>\n",
       "      <td>neutral</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>security and safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012_nyp_foreign_003</td>\n",
       "      <td>2012</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>deductions and loopholes; he hasn't been able ...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>tax_policy</td>\n",
       "      <td>tax_cuts_policy_proposals</td>\n",
       "      <td>negative</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>economic consequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012_nyp_foreign_004</td>\n",
       "      <td>2012</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>repeatedly hit romney for turning medicare int...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>healthcare_social_security</td>\n",
       "      <td>affordable_care_health_insurance</td>\n",
       "      <td>negative</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>economic consequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012_nyp_foreign_005</td>\n",
       "      <td>2012</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>winning the first debate is one thing; the ele...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>debate_format_procedure</td>\n",
       "      <td>debate_opening_closing_remarks</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>economic consequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012_nyp_foreign_006</td>\n",
       "      <td>2012</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>his term, saying iran \"saw weakness where it h...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>iran_nuclear_program</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>joy</td>\n",
       "      <td>security and safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012_nyp_foreign_007</td>\n",
       "      <td>2012</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>evidence the motive was terrorism. obama said ...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>patriot_act_homeland_security</td>\n",
       "      <td>negative</td>\n",
       "      <td>anger</td>\n",
       "      <td>security and safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2012_nyp_foreign_008</td>\n",
       "      <td>2012</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>evidence the motive was terrorism. obama said ...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>patriot_act_homeland_security</td>\n",
       "      <td>negative</td>\n",
       "      <td>anger</td>\n",
       "      <td>security and safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012_nyp_foreign_009</td>\n",
       "      <td>2012</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>\"netanyahu isn't stupid. he isn't going to sta...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>iran_nuclear_program</td>\n",
       "      <td>neutral</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>unspecified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2012_nyp_foreign_010</td>\n",
       "      <td>2012</td>\n",
       "      <td>nyp</td>\n",
       "      <td>R</td>\n",
       "      <td>in the last three years, food and gas prices h...</td>\n",
       "      <td>foreign_policy_national_security</td>\n",
       "      <td>partisan_gridlock_new_leadership</td>\n",
       "      <td>race_in_political_discourse</td>\n",
       "      <td>negative</td>\n",
       "      <td>unspecified</td>\n",
       "      <td>economic consequences</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               chunk_id  year outlet outlet_leaning  \\\n",
       "0  2012_nyp_foreign_001  2012    nyp              R   \n",
       "1  2012_nyp_foreign_002  2012    nyp              R   \n",
       "2  2012_nyp_foreign_003  2012    nyp              R   \n",
       "3  2012_nyp_foreign_004  2012    nyp              R   \n",
       "4  2012_nyp_foreign_005  2012    nyp              R   \n",
       "5  2012_nyp_foreign_006  2012    nyp              R   \n",
       "6  2012_nyp_foreign_007  2012    nyp              R   \n",
       "7  2012_nyp_foreign_008  2012    nyp              R   \n",
       "8  2012_nyp_foreign_009  2012    nyp              R   \n",
       "9  2012_nyp_foreign_010  2012    nyp              R   \n",
       "\n",
       "                                                text  \\\n",
       "0  the debate, in boca raton, fla., is supposed t...   \n",
       "1  the debate, in boca raton, fla., is supposed t...   \n",
       "2  deductions and loopholes; he hasn't been able ...   \n",
       "3  repeatedly hit romney for turning medicare int...   \n",
       "4  winning the first debate is one thing; the ele...   \n",
       "5  his term, saying iran \"saw weakness where it h...   \n",
       "6  evidence the motive was terrorism. obama said ...   \n",
       "7  evidence the motive was terrorism. obama said ...   \n",
       "8  \"netanyahu isn't stupid. he isn't going to sta...   \n",
       "9  in the last three years, food and gas prices h...   \n",
       "\n",
       "                       source_theme                             theme  \\\n",
       "0  foreign_policy_national_security  foreign_policy_national_security   \n",
       "1  foreign_policy_national_security  foreign_policy_national_security   \n",
       "2  foreign_policy_national_security                        tax_policy   \n",
       "3  foreign_policy_national_security        healthcare_social_security   \n",
       "4  foreign_policy_national_security           debate_format_procedure   \n",
       "5  foreign_policy_national_security  foreign_policy_national_security   \n",
       "6  foreign_policy_national_security  foreign_policy_national_security   \n",
       "7  foreign_policy_national_security  foreign_policy_national_security   \n",
       "8  foreign_policy_national_security  foreign_policy_national_security   \n",
       "9  foreign_policy_national_security  partisan_gridlock_new_leadership   \n",
       "\n",
       "                           subtheme    sentiment      emotion  \\\n",
       "0     patriot_act_homeland_security      neutral  unspecified   \n",
       "1     patriot_act_homeland_security      neutral  unspecified   \n",
       "2         tax_cuts_policy_proposals     negative  unspecified   \n",
       "3  affordable_care_health_insurance     negative  unspecified   \n",
       "4    debate_opening_closing_remarks  unspecified  unspecified   \n",
       "5              iran_nuclear_program  unspecified          joy   \n",
       "6     patriot_act_homeland_security     negative        anger   \n",
       "7     patriot_act_homeland_security     negative        anger   \n",
       "8              iran_nuclear_program      neutral  unspecified   \n",
       "9       race_in_political_discourse     negative  unspecified   \n",
       "\n",
       "                 framing  \n",
       "0    security and safety  \n",
       "1    security and safety  \n",
       "2  economic consequences  \n",
       "3  economic consequences  \n",
       "4  economic consequences  \n",
       "5    security and safety  \n",
       "6    security and safety  \n",
       "7    security and safety  \n",
       "8            unspecified  \n",
       "9  economic consequences  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === RENUMBER CHUNK IDS WITH METADATA ===\n",
    "\n",
    "df = pd.read_csv(DATA_DIR / \"media_full.csv\")\n",
    "\n",
    "# simplify source_theme (take prefix before \"_\")\n",
    "df[\"theme_simple\"] = df[\"source_theme\"].str.split(\"_\").str[0]\n",
    "\n",
    "# sort for stable numbering\n",
    "df = df.sort_values([\"year\", \"outlet\", \"theme_simple\"]).reset_index(drop=True)\n",
    "\n",
    "# assign group counter\n",
    "df[\"counter\"] = df.groupby([\"year\", \"outlet\", \"theme_simple\"]).cumcount() + 1\n",
    "df[\"counter\"] = df[\"counter\"].apply(lambda x: f\"{x:03d}\")\n",
    "\n",
    "# build new chunk_id\n",
    "df[\"chunk_id\"] = (\n",
    "    df[\"year\"].astype(str)\n",
    "    + \"_\" + df[\"outlet\"]\n",
    "    + \"_\" + df[\"theme_simple\"]\n",
    "    + \"_\" + df[\"counter\"]\n",
    ")\n",
    "\n",
    "# drop helper cols\n",
    "df = df.drop(columns=[\"theme_simple\", \"counter\"])\n",
    "\n",
    "# save updated file\n",
    "MEDIA_FULL = DATA_DIR / \"media_full.csv\"\n",
    "df.to_csv(MEDIA_FULL, index=False)\n",
    "\n",
    "print(\"Updated media_full.csv with new chunk_id:\", MEDIA_FULL)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR / \"media_balanced.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
